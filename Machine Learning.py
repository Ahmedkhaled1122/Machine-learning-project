import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from scipy import stats
from scipy.stats.mstats import winsorize
from scipy.stats import boxcox
import io
from packaging import version
import missingno as msno
import category_encoders as ce
import traceback


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC, SVR
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    mean_squared_error, mean_absolute_error, r2_score,
    silhouette_score, davies_bouldin_score
)

st.set_page_config(page_title="Data Preprocessing & ML Tool", layout="wide")

st.title("ðŸ“Š Data Preprocessing & Machine Learning Tool")

with st.sidebar:
    st.header("Upload Your Data")
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    
    if uploaded_file is not None:
        st.success("File uploaded successfully!")
        df = pd.read_csv(uploaded_file)

    else:
        st.info("Please upload a CSV file to get started.")
        st.stop()


def show_missing_value_percent(df):
    st.subheader("ðŸ•³ Missing Values (% per column)")
    missing_ratio = df.isnull().sum() / len(df) * 100
    missing_ratio = missing_ratio[missing_ratio > 0].sort_values(ascending=False)

    if missing_ratio.empty:
        st.success("âœ… No missing values in the dataset.")
    else:
        st.dataframe(missing_ratio.to_frame(name="Missing %"))
        st.info(f"ðŸ”¢ Number of columns with missing values: {len(missing_ratio)}")
        
        
st.header("ðŸ“Š Data Overview")
st.subheader("Original Data")
st.dataframe(df.head())
st.subheader("ðŸ§¾ Data Info")
buffer = io.StringIO()
df.info(buf=buffer)
info_str = buffer.getvalue()
st.code(info_str, language='text')
st.subheader(" ðŸ§® Data Description")
st.dataframe(df.describe())   
    
st.header("Duplicate Values")

try:
        duplicate_count = df.duplicated().sum()
        if duplicate_count > 0:
            st.warning(f"âš  Number of duplicate rows: {duplicate_count}")
            st.dataframe(df[df.duplicated()])
        else:
            st.success("âœ… No duplicate values found.")
except Exception as e:
        st.error(f"âŒ Error: {e}")



try:
        duplicate_count = df.duplicated().sum()
        if duplicate_count > 0:
            df = df.drop_duplicates()
            st.success(f"ðŸ§¹ Removed {duplicate_count} duplicate rows successfully!")
        else:
            st.success("âœ… No duplicate values found.")
except Exception as e:
        st.error(f"âŒ Error while removing duplicates: {e}")
      
show_missing_value_percent(df)

st.subheader("Drop Columns with Missing Values")

columns_to_drop = st.multiselect(
        "Select columns to drop (delete)", 
        options=df.columns,
        help="Select multiple columns to remove from the dataset"
    )


st.button("Drop Selected Columns")
if columns_to_drop:
    df.drop(columns=columns_to_drop, inplace=True, axis=1)
    st.success(f"Successfully dropped columns: {', '.join(columns_to_drop)}")
            
        

st.dataframe(df.head())
            
          


st.header("Missing Values Visualization")

option = st.radio(
    "Choose way for visualization",
    ("Matrix", "Heatmap", "Bar Chart"),
    horizontal=True
)

if option == "Matrix":
    fig, ax = plt.subplots(figsize=(10, 6))
    msno.matrix(df, ax=ax, color=(0.53, 0.81, 0.92))
    ax.set_title("Missing Values Matrix")
    st.pyplot(fig)

if option == "Heatmap":
    fig, ax = plt.subplots(figsize=(10, 6))
    msno.heatmap(df, ax=ax, cmap='magma')
    ax.set_title("Missing Values Heatmap")
    st.pyplot(fig)

if option == "Bar Chart":
    fig, ax = plt.subplots(figsize=(10, 6))
    msno.bar(df, ax=ax, color='teal')
    ax.set_title("Missing Values Bar Chart")
    st.pyplot(fig)

missing_cols = df.columns[df.isnull().any()].tolist()
df_missing = df[missing_cols]

if len(missing_cols) > 0:
    st.subheader("Distribution of Columns with Missing Values")
    st.write("Histograms to guide imputation strategy:")
    
    for col in missing_cols:
        st.markdown(f"**{col}** (Missing: {df[col].isnull().sum()} values)")
        
        fig, ax = plt.subplots(figsize=(8, 3))
        
        if pd.api.types.is_numeric_dtype(df[col]):
            sns.histplot(df[col], kde=True, color="skyblue", bins=30)
            plt.title(f"Distribution of {col}")
            plt.xlabel(col)
        else:
            sns.countplot(x=df[col], palette="viridis")
            plt.title(f"Categories in {col}")
            plt.xticks(rotation=45)
        
        st.pyplot(fig)
        st.write("---")
else:
    st.success("No missing values found in the dataset!")


st.header("3. Handle Missing Values")
missing_cols = df.columns[df.isnull().any()].tolist()


    
missing_stats_before = pd.DataFrame({
        'Column': missing_cols,
        'Missing Count': [df[col].isnull().sum() for col in missing_cols],
        'Missing %': [round(df[col].isnull().mean()*100, 2) for col in missing_cols],
        'Data Type': [df[col].dtype for col in missing_cols]
    }).sort_values('Missing Count', ascending=False)
    
st.write("### Missing Values Statistics (Before)")
st.dataframe(missing_stats_before.style.format({'Missing %': '{:.2f}%'}))

cols_to_impute = st.multiselect(
        "Select columns to impute", 
        options=missing_cols,
        default=missing_cols,
        help="Choose which columns to apply missing value imputation"
    )

imputation_settings = {}
    
for col in cols_to_impute:
        st.markdown(f"### Column: {col}")
        st.write(f"- Current missing values: {df[col].isnull().sum()} ({round(df[col].isnull().mean()*100, 2)}%)")
        
        strategy = None
        fill_value = None
        n_neighbors = None
        max_iter = None
        
        col_type = "numeric" if pd.api.types.is_numeric_dtype(df[col]) else "categorical"
        
        col1, col2 = st.columns(2)
        
        with col1:
            if col_type == "numeric":
                method = st.selectbox(
                    f"Method for {col}",
                    ["Simple Imputer", "KNN Imputer", "Iterative Imputer", "Fill with value"],
                    key=f"method_{col}"
                )
            else:
                method = st.selectbox(
                    f"Method for {col}",
                    ["Simple Imputer", "Fill with value"],
                    key=f"method_{col}"
                )
        
        with col2:
            if method == "Simple Imputer":
                if col_type == "numeric":
                    strategy = st.selectbox(
                        f"Strategy for {col}",
                        ["mean", "median", "most_frequent", "constant"],
                        key=f"strategy_{col}"
                    )
                    if strategy == "constant":
                        fill_value = st.number_input(
                            f"Fill value for {col}",
                            value=0,
                            key=f"fill_{col}"
                        )
                else:
                    strategy = st.selectbox(
                        f"Strategy for {col}",
                        ["most_frequent", "constant"],
                        key=f"strategy_{col}"
                    )
                    if strategy == "constant":
                        fill_value = st.text_input(
                            f"Fill value for {col}",
                            value="UNKNOWN",
                            key=f"fill_{col}"
                        )
            
            elif method == "Fill with value":
                if col_type == "numeric":
                    fill_value = st.number_input(
                        f"Fill value for {col}",
                        value=0,
                        key=f"fill_{col}"
                    )
                else:
                    fill_value = st.text_input(
                        f"Fill value for {col}",
                        value="UNKNOWN",
                        key=f"fill_{col}"
                    )
            
            elif method == "KNN Imputer":
                n_neighbors = st.number_input(
                    f"Number of neighbors for {col}",
                    min_value=1,
                    value=5,
                    key=f"neighbors_{col}"
                )
            
            elif method == "Iterative Imputer":
                max_iter = st.number_input(
                    f"Max iterations for {col}",
                    min_value=1,
                    value=10,
                    key=f"iter_{col}"
                )
        
        imputation_settings[col] = {
            "type": col_type,
            "method": method,
            "strategy": strategy,
            "fill_value": fill_value,
            "n_neighbors": n_neighbors,
            "max_iter": max_iter
        }



simple_numeric = []
simple_categorical = []
fill_values = {}
knn_cols = []
iterative_cols = []

for col, settings in imputation_settings.items():
    if settings["method"] == "Simple Imputer":
        if settings["type"] == "numeric":
            simple_numeric.append((col, settings["strategy"], settings["fill_value"]))
        else:
            simple_categorical.append((col, settings["strategy"], settings["fill_value"]))
    elif settings["method"] == "Fill with value":
        fill_values[col] = settings["fill_value"]
    elif settings["method"] == "KNN Imputer":
        knn_cols.append(col)
    elif settings["method"] == "Iterative Imputer":
        iterative_cols.append(col)

for col, strategy, fill_value in simple_numeric:
    if strategy == "constant":
        imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)
    else:
        imputer = SimpleImputer(strategy=strategy)
    df[[col]] = imputer.fit_transform(df[[col]])

for col, strategy, fill_value in simple_categorical:
    if strategy == "constant":
        imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)
    else:
        imputer = SimpleImputer(strategy=strategy)
    df[[col]] = imputer.fit_transform(df[[col]])

for col, value in fill_values.items():
    df[col] = df[col].fillna(value)

if knn_cols:
    n_neighbors = imputation_settings[knn_cols[0]]["n_neighbors"]
    knn_imputer = KNNImputer(n_neighbors=n_neighbors)
    df[knn_cols] = knn_imputer.fit_transform(df[knn_cols])

if iterative_cols:
    max_iter = imputation_settings[iterative_cols[0]]["max_iter"]
    iterative_imputer = IterativeImputer(max_iter=max_iter, random_state=42)
    df[iterative_cols] = iterative_imputer.fit_transform(df[iterative_cols])

st.subheader("Imputation Results")

missing_stats_after = pd.DataFrame({
    'Column': cols_to_impute,
    'Missing Before': [df[col].isnull().sum() for col in cols_to_impute],
    'Missing After': [df[col].isnull().sum() for col in cols_to_impute],
    'Method': [imputation_settings[col]["method"] for col in cols_to_impute],
    'Status': ["âœ… Completed" if df[col].isnull().sum() == 0 
            else "âš  Partial" for col in cols_to_impute]
})

st.write("### Missing Values Count Comparison")
st.dataframe(missing_stats_after)

if df.isnull().any().any():
    st.subheader(" Missing Values After Imputation")
    fig_after, ax_after = plt.subplots(figsize=(12, 6))
    msno.matrix(df, ax=ax_after, color=(0.8, 0.3, 0.3))
    ax_after.set_title("Remaining Missing Values", fontsize=14)
    st.pyplot(fig_after)
    plt.close(fig_after)
else:
    st.success("ðŸŽ‰ All missing values have been successfully imputed!")
    
    st.subheader(" Missing Values After Imputation")
    fig_after, ax_after = plt.subplots(figsize=(12, 6))
    msno.matrix(df, ax=ax_after, color=(0.8, 0.3, 0.3))
    ax_after.set_title("Remaining Missing Values", fontsize=14)
    st.pyplot(fig_after)
    plt.close(fig_after)
        
        
        



df_num = df.select_dtypes(include=[np.number])

st.subheader("ðŸ“¦ show outliers")

if not df_num.empty:
    num_cols = df_num.columns
    
    fig, axes = plt.subplots(nrows=1, ncols=len(num_cols), figsize=(5 * len(num_cols), 5))
    
    if len(num_cols) == 1:
        axes = [axes]
                
    for i, col in enumerate(num_cols):
        sns.boxplot(data=df_num, y=col, ax=axes[i], color="#b480b8")
        axes[i].set_title(f'Boxplot of {col}', fontsize=14)
    
    plt.tight_layout()
    st.pyplot(fig)
    
    
    

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()


st.header("5-6. Outlier Detection & Handling")

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

if not numeric_cols:
    st.warning("No numeric columns found for outlier detection!")
else:
    selected_cols = st.multiselect(
        "Select columns for outlier handling",
        options=numeric_cols,
        default=numeric_cols,
        help="Choose which numeric columns to analyze and process"
    )
    
    if not selected_cols:
        st.warning("Please select at least one column")
        st.stop()
    
    out_col1, out_col2 = st.columns(2)
    
    with out_col1:
        outlier_method = st.selectbox("Select outlier detection method", ['Z-Score', 'IQR'])
    
    with out_col2:
        handle_method = st.selectbox("Choose how to handle outliers", 
                                ['Remove outliers', 'Winsorization', 'Clip outliers'])


original_row_count = len(df)
results = []
        
fig, axes = plt.subplots(len(selected_cols), 2, figsize=(15, 5*len(selected_cols)))

if len(selected_cols) == 1:
    axes = [axes]

for i, col in enumerate(selected_cols):
    sns.boxplot(y=df[col], ax=axes[i][0], color='skyblue')
    axes[i][0].set_title(f"{col} (Before)")
    
    if outlier_method == 'Z-Score':
        z_scores = np.abs(stats.zscore(df[col]))
        outliers_mask = z_scores > 3
        lower_bound = None
        upper_bound = None
    else:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
    
    outlier_count = sum(outliers_mask)
    
    if handle_method == 'Remove outliers':
        if outlier_method == 'Z-Score':
            df = df[z_scores <= 3]
        else:
            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
        action = "removed"
    elif handle_method == 'Winsorization':
        df[col] = winsorize(df[col], limits=[0.05, 0.05])
        action = "winsorized"
    else:
        if lower_bound is not None and upper_bound is not None:
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        action = "clipped"
    
    sns.boxplot(y=df[col], ax=axes[i][1], color='lightgreen')
    axes[i][1].set_title(f"{col} (After {action})")
    
    

plt.tight_layout()

st.subheader("Before vs After Outlier Handling")
st.pyplot(fig)


if handle_method == 'Remove outliers':
    new_row_count = len(df)
    st.write(f"ðŸ“Š Row count before: {original_row_count} | After: {new_row_count}")
    st.write(f"ðŸ“‰ Rows removed: {original_row_count - new_row_count}")
    


st.success(f"Outlier handling completed for {len(selected_cols)} selected columns!")



st.header("ðŸ“Š Data Visualization")

viz_tab1, viz_tab2 = st.tabs(["Numerical Data", "Categorical Data"])

with viz_tab1:
    st.subheader("Numerical Data Visualization")
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    
    if not numeric_cols:
        st.warning("No numerical columns found for visualization!")
    else:
        st.markdown("### Single Variable Analysis")
        num_col1 = st.selectbox("Select numerical column", numeric_cols, key='num_col1')
        
        plot_type = st.radio("Select plot type", 
                        ["Histogram", "Density Plot", "Box Plot", "Violin Plot"], 
                        key='num_plot_type')
        
        if st.button("Generate Numerical Plot"):
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if plot_type == "Histogram":
                sns.histplot(df[num_col1], ax=ax, kde=False, color='skyblue')
                ax.set_title(f"Histogram of {num_col1}")
            elif plot_type == "Density Plot":
                sns.kdeplot(df[num_col1], ax=ax, color='purple', fill=True)
                ax.set_title(f"Density Plot of {num_col1}")
            elif plot_type == "Box Plot":
                sns.boxplot(y=df[num_col1], ax=ax, color='lightgreen')
                ax.set_title(f"Box Plot of {num_col1}")
            elif plot_type == "Violin Plot":
                sns.violinplot(y=df[num_col1], ax=ax, color='orange')
                ax.set_title(f"Violin Plot of {num_col1}")
            
            st.pyplot(fig)
            plt.close(fig)
        
        st.markdown("### Two Variable Analysis")
        col1, col2 = st.columns(2)
        with col1:
            num_col_x = st.selectbox("Select X-axis column", numeric_cols, key='num_col_x')
        with col2:
            num_col_y = st.selectbox("Select Y-axis column", numeric_cols, key='num_col_y')
        
        two_var_plot_type = st.radio("Select plot type", 
                                    ["Scatter Plot", "Line Plot", "Hexbin Plot"], 
                                    key='two_var_plot')
        
        if st.button("Generate Two Variable Plot"):
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if two_var_plot_type == "Scatter Plot":
                sns.scatterplot(data=df, x=num_col_x, y=num_col_y, ax=ax, alpha=0.7)
                ax.set_title(f"Scatter Plot: {num_col_x} vs {num_col_y}")
            elif two_var_plot_type == "Line Plot":
                sns.lineplot(data=df, x=num_col_x, y=num_col_y, ax=ax)
                ax.set_title(f"Line Plot: {num_col_x} vs {num_col_y}")
            elif two_var_plot_type == "Hexbin Plot":
                plt.hexbin(df[num_col_x], df[num_col_y], gridsize=25, cmap='Blues')
                plt.colorbar()
                ax.set_title(f"Hexbin Plot: {num_col_x} vs {num_col_y}")
            
            st.pyplot(fig)
            plt.close(fig)

with viz_tab2:
    st.subheader("Categorical Data Visualization")
    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if not cat_cols:
        st.warning("No categorical columns found for visualization!")
    else:
        st.markdown("### Single Variable Analysis")
        cat_col = st.selectbox("Select categorical column", cat_cols, key='cat_col')
        
        cat_plot_type = st.radio("Select plot type", 
                               ["Bar Plot", "Pie Chart", "Lollipop Plot", "Treemap"],
                               key='cat_plot_type')
        
        if st.button("Generate Categorical Plot"):
            fig, ax = plt.subplots(figsize=(10, 6))
            value_counts = df[cat_col].value_counts()
            
            if cat_plot_type == "Bar Plot":
                sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax, palette='viridis')
                ax.set_title(f"Bar Plot of {cat_col}")
                ax.tick_params(axis='x', rotation=45)
            elif cat_plot_type == "Pie Chart":
                ax.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', 
                      startangle=90, colors=sns.color_palette('pastel'))
                ax.set_title(f"Pie Chart of {cat_col}")
            elif cat_plot_type == "Lollipop Plot":
                ax.stem(value_counts.index, value_counts.values, basefmt=" ")
                ax.set_title(f"Lollipop Plot of {cat_col}")
                ax.tick_params(axis='x', rotation=45)
            elif cat_plot_type == "Treemap":
                import squarify
                squarify.plot(sizes=value_counts.values, 
                            label=value_counts.index, 
                            color=sns.color_palette('Spectral', len(value_counts)))
                ax.set_title(f"Treemap of {cat_col}")
                ax.axis('off')
            
            st.pyplot(fig)
            plt.close(fig)
        
        st.markdown("### Categorical vs Numerical Analysis")
        col1, col2 = st.columns(2)
        with col1:
            cat_col_x = st.selectbox("Select categorical column", cat_cols, key='cat_col_x')
        with col2:
            num_col_y = st.selectbox("Select numerical column", numeric_cols, key='num_col_y2')
        
        cat_num_plot_type = st.radio("Select plot type", 
                                   ["Box Plot", "Violin Plot", "Swarm Plot", "Bar Plot"],
                                   key='cat_num_plot')
        
        if st.button("Generate Categorical-Numerical Plot"):
            fig, ax = plt.subplots(figsize=(10, 6))
            
            if cat_num_plot_type == "Box Plot":
                sns.boxplot(data=df, x=cat_col_x, y=num_col_y, ax=ax, palette='Set2')
                ax.set_title(f"Box Plot: {cat_col_x} vs {num_col_y}")
            elif cat_num_plot_type == "Violin Plot":
                sns.violinplot(data=df, x=cat_col_x, y=num_col_y, ax=ax, palette='Set2')
                ax.set_title(f"Violin Plot: {cat_col_x} vs {num_col_y}")
            elif cat_num_plot_type == "Swarm Plot":
                sns.swarmplot(data=df, x=cat_col_x, y=num_col_y, ax=ax, palette='Set2')
                ax.set_title(f"Swarm Plot: {cat_col_x} vs {num_col_y}")
            elif cat_num_plot_type == "Bar Plot":
                sns.barplot(data=df, x=cat_col_x, y=num_col_y, ax=ax, palette='Set2', 
                           estimator=np.mean, ci=95)
                ax.set_title(f"Bar Plot (Mean): {cat_col_x} vs {num_col_y}")
            
            ax.tick_params(axis='x', rotation=45)
            st.pyplot(fig)
            plt.close(fig)



st.header("ðŸ”— Correlation Analysis")

if not numeric_cols:
    st.warning("No numerical columns found for correlation analysis!")
else:
    st.markdown("### Correlation Matrix")
    
    corr_matrix = df[numeric_cols].corr()
    
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax)
    ax.set_title("Correlation Matrix of Numerical Features")
    st.pyplot(fig)
    plt.close(fig)
    
    st.markdown("### Top Correlated Pairs")
    
    corr_pairs = corr_matrix.unstack().sort_values(ascending=False)
    top_pairs = corr_pairs[corr_pairs != 1].head(10)
    
    st.write("Top 10 most correlated feature pairs:")
    st.dataframe(top_pairs.reset_index().rename(columns={'level_0': 'Feature 1', 
                                                      'level_1': 'Feature 2', 
                                                      0: 'Correlation'}))







st.header("4. Categorical Data Encoding")

categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

if categorical_cols:
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Select Columns and Encoding Method")

        encoding_settings = {}

        for col in categorical_cols:
            method = st.selectbox(
                f"Encoding method for column *{col}*",
                ['None', 'Label Encoding', 'One-Hot Encoding', 'Binary Encoding', 'Most Frequent Encoding'],
                key=f"encode_{col}"
            )
            encoding_settings[col] = method
    with col2:
        st.subheader("Preview & Apply")


try:
        for col, method in encoding_settings.items():
            if method == 'Label Encoding':
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col])
                st.success(f"âœ… Label Encoding applied to '{col}'")

            elif method == 'One-Hot Encoding':
                ohe = OneHotEncoder(sparse_output=False)
                encoded = ohe.fit_transform(df[[col]])
                new_cols = ohe.get_feature_names_out([col])
                df_encoded = pd.DataFrame(encoded, columns=new_cols, index=df.index)
                df.drop(columns=[col], inplace=True)
                df = pd.concat([df, df_encoded], axis=1)
                st.success(f"âœ… Oneâ€‘Hot Encoding applied to '{col}'")

            elif method == 'Binary Encoding':
                be = ce.BinaryEncoder(cols=[col])
                df = be.fit_transform(df)
                st.success(f"âœ… Binary Encoding applied to '{col}'")

            elif method == 'Most Frequent Encoding':
                ce_count = ce.CountEncoder(cols=[col], normalize=True)
                df[f"{col}_MostFreq"] = ce_count.fit_transform(df[[col]])
                df.drop(columns=[col], inplace=True)
                st.success(f"âœ… Most-Frequent Encoding applied to '{col}'")


        st.success("ðŸŽ‰ All selected encodings applied successfully!")

except Exception as e:
        st.error(f"âš  Error: {e}")


st.dataframe(df.head())


st.header("7-8. Feature Scaling")

scale_col1, scale_col2 = st.columns(2)

with scale_col1:
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    scale_columns = st.multiselect("Select numeric columns to scale", 
                                    numeric_cols if numeric_cols else [])

with scale_col2:
    scale_method = st.selectbox("Select scaling method", 
                                ['Normalization (MinMaxScaler)', 'Standardization (StandardScaler)'])

if len(scale_columns) > 0:
    if scale_method == 'Normalization (MinMaxScaler)':
        scaler = MinMaxScaler()
        df[scale_columns] = scaler.fit_transform(df[scale_columns])
        st.success("Applied MinMax scaling to selected columns")
        st.dataframe(df.head())
    else:
        scaler = StandardScaler()
        df[scale_columns] = scaler.fit_transform(df[scale_columns])
        st.success("Applied Standard scaling to selected columns")
        st.dataframe(df.head())

        
st.header("9-10. Skewness Analysis & Transformation")

skew_col1, skew_col2 = st.columns(2)

with skew_col1:
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    skew_column = st.selectbox("Select numeric column for skewness analysis", 
                                ['None'] + numeric_cols if numeric_cols else ['None'])

if skew_column != 'None':
    skewness = df[skew_column].skew()
    st.write(f"Skewness of {skew_column}: {skewness:.2f}")
    
    if abs(skewness) < 0.5:
        st.success("The distribution is approximately symmetric")
    elif 0.5 <= abs(skewness) < 1:
        st.warning("The distribution is moderately skewed")
    else:
        st.error("The distribution is highly skewed")
    
    fig, ax = plt.subplots(figsize=(10, 4))
    sns.histplot(df[skew_column], kde=True, ax=ax,color='#ef8d56')
    plt.title(f'Distribution of {skew_column}')
    st.pyplot(fig)
    
    with skew_col2:
        transform_method = st.selectbox("Select transformation method", 
                                        ['Log Transformation', 'Box-Cox Transformation'])
    
    st.button("Apply Transformation")
    if transform_method == 'Log Transformation':
            if (df[skew_column] <= 0).any():
                st.warning("Log transform requires positive values. Adding constant to make values positive.")
                constant = abs(df[skew_column].min()) + 1
                df[skew_column] = np.log(df[skew_column] + constant)
            else:
                df[skew_column] = np.log(df[skew_column])
            st.success("Applied log transformation")
    else:
            if (df[skew_column] <= 0).any():
                st.error("Box-Cox requires strictly positive values. Cannot apply transformation.")
            else:
                df[skew_column], _ = boxcox(df[skew_column])
                st.success("Applied Box-Cox transformation")
        
    new_skewness = df[skew_column].skew()
    st.write(f"New skewness: {new_skewness:.2f}")
        
    fig, ax = plt.subplots(figsize=(10, 4))
    sns.histplot(df[skew_column], kde=True, ax=ax)
    plt.title(f'Distribution after transformation')
    st.pyplot(fig)
        
        



st.header("Machine Learning Models")


problem_type = st.radio("Select problem type", ["Classification", "Regression", "Clustering"])

if problem_type in ["Classification", "Regression"]:
    target_col = st.selectbox("Select target variable", df.columns)
    
    if not target_col:
        st.warning("Please select a target column first.")
        st.stop()
    
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
    categorical_features = X.select_dtypes(include=['object', 'category']).columns
    
    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])
    
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])
    
    if problem_type == "Classification":
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
    
    test_size = st.slider("Test size ratio", 0.1, 0.5, 0.2, 0.05)
    random_state = st.slider("Random state", 0, 100, 42)
    
    try:
        if problem_type == "Classification":
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, 
                test_size=test_size, 
                random_state=random_state,
                stratify=y_encoded if len(np.unique(y_encoded)) > 1 else None
            )
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=test_size, 
                random_state=random_state
            )
    except ValueError as e:
        st.error(f"Error in train-test split: {str(e)}")
        st.stop()

if problem_type == "Classification":
    st.subheader("Classification Models")
    model_name = st.selectbox("Select model", 
                            ["SVM", "Random Forest", "Naive Bayes", 
                            "Logistic Regression", "KNN", "Decision Tree"])
    
    model_params = {}
    if model_name == "SVM":
        model_params['C'] = st.slider("Regularization parameter (C)", 0.01, 10.0, 1.0)
        model_params['kernel'] = st.selectbox("Kernel", ["linear", "rbf", "poly", "sigmoid"])
        model = SVC(**model_params, probability=True)
    
    elif model_name == "Random Forest":
        model_params['n_estimators'] = st.slider("Number of trees", 10, 200, 100)
        model_params['max_depth'] = st.slider("Max depth", 1, 20, 5)
        model = RandomForestClassifier(**model_params, random_state=42)
    
    elif model_name == "Naive Bayes":
        model = GaussianNB()
    
    elif model_name == "Logistic Regression":
        model_params['C'] = st.slider("Inverse of regularization strength (C)", 
                                    0.01, 10.0, 1.0)
        model_params['penalty'] = st.selectbox("Penalty", ["l2", "none"])
        model = LogisticRegression(**model_params, random_state=42, max_iter=1000)
    
    elif model_name == "KNN":
        model_params['n_neighbors'] = st.slider("Number of neighbors", 1, 20, 5)
        model_params['weights'] = st.selectbox("Weights", ["uniform", "distance"])
        model = KNeighborsClassifier(**model_params)
    
    elif model_name == "Decision Tree":
        model_params['max_depth'] = st.slider("Max depth", 1, 20, 5)
        model_params['criterion'] = st.selectbox("Criterion", ["gini", "entropy"])
        model = DecisionTreeClassifier(**model_params, random_state=42)

elif problem_type == "Regression":
    st.subheader("Regression Models")
    model_name = st.selectbox("Select model", 
                            ["Linear Regression", "SVR", "Random Forest", 
                            "KNN", "Decision Tree"])
    
    model_params = {}
    if model_name == "Linear Regression":
        model = LinearRegression()
    
    elif model_name == "SVR":
        model_params['C'] = st.slider("Regularization parameter (C)", 0.01, 10.0, 1.0)
        model_params['kernel'] = st.selectbox("Kernel", ["linear", "rbf", "poly", "sigmoid"])
        model = SVR(**model_params)
    
    elif model_name == "Random Forest":
        model_params['n_estimators'] = st.slider("Number of trees", 10, 200, 100)
        model_params['max_depth'] = st.slider("Max depth", 1, 20, 5)
        model = RandomForestRegressor(**model_params, random_state=42)
    
    elif model_name == "KNN":
        model_params['n_neighbors'] = st.slider("Number of neighbors", 1, 20, 5)
        model_params['weights'] = st.selectbox("Weights", ["uniform", "distance"])
        model = KNeighborsRegressor(**model_params)
    
    elif model_name == "Decision Tree":
        model_params['max_depth'] = st.slider("Max depth", 1, 20, 5)
        model_params['criterion'] = st.selectbox("Criterion", ["squared_error", "friedman_mse"])
        model = DecisionTreeRegressor(**model_params, random_state=42)

elif problem_type == "Clustering":
    st.subheader("Clustering Models")
    model_name = st.selectbox("Select model", 
                            ["K-Means", "DBSCAN", "Agglomerative"])
    
    model_params = {}
    if model_name == "K-Means":
        model_params['n_clusters'] = st.slider("Number of clusters", 2, 10, 3)
        model = KMeans(**model_params, random_state=42)
    
    elif model_name == "DBSCAN":
        model_params['eps'] = st.slider("Epsilon (eps)", 0.1, 2.0, 0.5, 0.1)
        model_params['min_samples'] = st.slider("Minimum samples", 1, 20, 5)
        model = DBSCAN(**model_params)
    
    elif model_name == "Agglomerative":
        model_params['n_clusters'] = st.slider("Number of clusters", 2, 10, 3)
        model_params['linkage'] = st.selectbox("Linkage", ["ward", "complete", "average", "single"])
        model = AgglomerativeClustering(**model_params)

if problem_type in ["Classification", "Regression"]:
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    if st.button("Train Model"):
        with st.spinner("Training model..."):
            model_pipeline.fit(X_train, y_train)
            
            if problem_type == "Classification":
                y_pred = model_pipeline.predict(X_test)
                y_prob = model_pipeline.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
                y_test_original = le.inverse_transform(y_test)
                y_pred_original = le.inverse_transform(y_pred)
                
                st.subheader("Classification Metrics")
                metrics_df = pd.DataFrame({
                    "Metric": ["Accuracy", "Precision", "Recall", "F1 Score"],
                    "Value": [
                        accuracy_score(y_test_original, y_pred_original),
                        precision_score(y_test_original, y_pred_original, average='weighted', zero_division=0),
                        recall_score(y_test_original, y_pred_original, average='weighted', zero_division=0),
                        f1_score(y_test_original, y_pred_original, average='weighted', zero_division=0)
                    ]
                })
                st.dataframe(metrics_df.style.format({"Value": "{:.4f}"}))
                
                if y_prob is not None and len(np.unique(y_test)) > 1:
                    try:
                        roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')
                        st.write(f"ROC AUC: {roc_auc:.4f}")
                    except Exception as e:
                        st.warning(f"Could not calculate ROC AUC: {str(e)}")

                st.subheader("Confusion Matrix")
                cm = confusion_matrix(y_test_original, y_pred_original)
                fig, ax = plt.subplots()
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                        xticklabels=le.classes_,
                        yticklabels=le.classes_)
                ax.set_xlabel('Predicted')
                ax.set_ylabel('Actual')
                st.pyplot(fig)
                
                st.subheader("Classification Report")
                report = classification_report(y_test_original, y_pred_original, output_dict=True)
                report_df = pd.DataFrame(report).transpose()
                st.dataframe(report_df.style.format("{:.4f}"))
            
            elif problem_type == "Regression":
                y_pred = model_pipeline.predict(X_test)
                
                st.subheader("Regression Metrics")
                metrics_df = pd.DataFrame({
                    "Metric": ["MSE", "RMSE", "MAE", "RÂ²"],
                    "Value": [
                        mean_squared_error(y_test, y_pred),
                        np.sqrt(mean_squared_error(y_test, y_pred)),
                        mean_absolute_error(y_test, y_pred),
                        r2_score(y_test, y_pred)
                    ]
                })
                st.dataframe(metrics_df.style.format({"Value": "{:.4f}"}))
                
                st.subheader("Regression Plot")
                fig, ax = plt.subplots()
                ax.scatter(y_test, y_pred, alpha=0.5)
                ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
                ax.set_xlabel('Actual')
                ax.set_ylabel('Predicted')
                st.pyplot(fig)

elif problem_type == "Clustering":
    if st.button("Apply Clustering"):
        with st.spinner("Clustering data..."):
            preprocessor = ColumnTransformer(
                transformers=[
                    ('num', StandardScaler(), numeric_features),
                    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
                ])
            
            X_processed = preprocessor.fit_transform(X)
            
            clusters = model.fit_predict(X_processed)
            
            df_clustered = df.copy()
            df_clustered['Cluster'] = clusters
            
            st.subheader("Clustering Results")
            
            if model_name != "DBSCAN" or len(np.unique(clusters)) > 1:
                try:
                    silhouette = silhouette_score(X_processed, clusters)
                    davies_bouldin = davies_bouldin_score(X_processed, clusters)
                    
                    metrics_df = pd.DataFrame({
                        "Metric": ["Silhouette Score", "Davies-Bouldin Index"],
                        "Value": [silhouette, davies_bouldin]
                    })
                    st.dataframe(metrics_df.style.format({"Value": "{:.4f}"}))
                except:
                    st.warning("Could not calculate clustering metrics for this configuration")
            
            st.subheader("Cluster Distribution")
            cluster_counts = pd.Series(clusters).value_counts().sort_index()
            st.bar_chart(cluster_counts)
            
            st.subheader("Clustered Data Preview")
            st.dataframe(df_clustered.head())
            
            if st.button("Download Clustered Data"):
                csv = df_clustered.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name="clustered_data.csv",
                    mime="text/csv"
                )